{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532db1f2-8aed-47b4-bc3b-d5642936534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055aaed4-fd76-481b-811e-bdb60c158da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/16 18:56:08 WARN Utils: Your hostname, Omars-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.45 instead (on interface en0)\n",
      "25/08/16 18:56:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/16 18:56:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"BinaryStringClassification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d93bf5d7-690e-46ad-8d14-afa3e1976e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('sentiment_analysis.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ad9dd8-5e8e-4c0f-89ef-611529ef632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"label\", col(\"label\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9bf034-feb6-4f6f-9b91-71d551bd004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df8204e-f10d-4d06-9994-f69ca7893c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 5894|\n",
      "|  1.0| 2026|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be0243f7-e30f-441e-8316-a45f1044cfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|null_or_empty_tweets|\n",
      "+--------------------+\n",
      "|                   0|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "df.select(\n",
    "    count(when(col(\"tweet\").isNull() | (col(\"tweet\") == \"\"), \"tweet\")).alias(\"null_or_empty_tweets\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "789c99ca-4840-4c00-8833-9262a8d6f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, trim\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"tweet\",\n",
    "    trim(\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                lower(col(\"tweet\")),             # lowercase\n",
    "                r\"http\\S+|@\\w+|#\\w+|[^a-zA-Z\\s]\", \"\"  # remove URLs, mentions, hashtags, special chars\n",
    "            ),\n",
    "            r\"\\bpictwitter\\w+\\b\", \"\"             # remove 'pictwitter...'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optional: replace multiple spaces with a single space\n",
    "df = df.withColumn(\"tweet\", regexp_replace(col(\"tweet\"), r\"\\s{2,}\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e924ed-2978-4e0f-89e6-c35619a804e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|                test|\n",
      "|finally a transpa...|\n",
      "|we love this woul...|\n",
      "|im wired i know i...|\n",
      "|what amazing serv...|\n",
      "|iphone software u...|\n",
      "|        happy for us|\n",
      "|new type c charge...|\n",
      "|bout to go shoppi...|\n",
      "|               photo|\n",
      "|hey when you make...|\n",
      "|ha not heavy mach...|\n",
      "|contemplating giv...|\n",
      "|i just made anoth...|\n",
      "|the battery is so...|\n",
      "|        from towards|\n",
      "|like and share if...|\n",
      "|            go crazy|\n",
      "|the reason i dont...|\n",
      "|how is the apple ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('tweet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538af757-7bc6-4f80-8ab3-8e2b22ec6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df=df.withColumn('length',length(df['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf47285c-4972-4b34-a609-ad70b95574a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+------+\n",
      "| id|label|               tweet|length|\n",
      "+---+-----+--------------------+------+\n",
      "|  1|  0.0|                test|     4|\n",
      "|  2|  0.0|finally a transpa...|    53|\n",
      "|  3|  0.0|we love this woul...|    25|\n",
      "|  4|  0.0|im wired i know i...|    45|\n",
      "|  5|  1.0|what amazing serv...|   114|\n",
      "|  6|  1.0|iphone software u...|    65|\n",
      "|  7|  0.0|        happy for us|    12|\n",
      "|  8|  0.0|new type c charge...|    45|\n",
      "|  9|  0.0|bout to go shoppi...|    44|\n",
      "| 10|  0.0|               photo|     5|\n",
      "| 11|  1.0|hey when you make...|   114|\n",
      "| 12|  1.0|ha not heavy mach...|    93|\n",
      "| 13|  1.0|contemplating giv...|    82|\n",
      "| 14|  0.0|i just made anoth...|    75|\n",
      "| 15|  1.0|the battery is so...|    80|\n",
      "| 16|  0.0|        from towards|    12|\n",
      "| 17|  0.0|like and share if...|    55|\n",
      "| 18|  0.0|            go crazy|     8|\n",
      "| 19|  1.0|the reason i dont...|    46|\n",
      "| 20|  1.0|how is the apple ...|    90|\n",
      "+---+-----+--------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "407cdf33-efb7-405f-b0fe-e7ea05740caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+------+\n",
      "| id|label|               tweet|length|\n",
      "+---+-----+--------------------+------+\n",
      "|  1|  0.0|                test|     4|\n",
      "|  2|  0.0|finally a transpa...|    53|\n",
      "|  3|  0.0|we love this woul...|    25|\n",
      "|  4|  0.0|im wired i know i...|    45|\n",
      "|  5|  1.0|what amazing serv...|   114|\n",
      "|  6|  1.0|iphone software u...|    65|\n",
      "|  7|  0.0|        happy for us|    12|\n",
      "|  8|  0.0|new type c charge...|    45|\n",
      "|  9|  0.0|bout to go shoppi...|    44|\n",
      "| 10|  0.0|               photo|     5|\n",
      "| 11|  1.0|hey when you make...|   114|\n",
      "| 12|  1.0|ha not heavy mach...|    93|\n",
      "| 13|  1.0|contemplating giv...|    82|\n",
      "| 14|  0.0|i just made anoth...|    75|\n",
      "| 15|  1.0|the battery is so...|    80|\n",
      "| 16|  0.0|        from towards|    12|\n",
      "| 17|  0.0|like and share if...|    55|\n",
      "| 18|  0.0|            go crazy|     8|\n",
      "| 19|  1.0|the reason i dont...|    46|\n",
      "| 20|  1.0|how is the apple ...|    90|\n",
      "+---+-----+--------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54e79119-0958-426d-b352-f14414af0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"toknized_words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a255b105-cd69-4981-bc88-a7a4eca2dfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/16 18:58:43 WARN StopWordsRemover: Default locale set was [en_EG]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e0da0c6-c70a-4eea-812d-d2831e2b9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"features\", numFeatures=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b51ef215-f1cd-4686-b60b-f049e39070ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "nv=NaiveBayes(featuresCol=\"features\",labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac9f6df3-4978-4f9c-a3fd-ea690b3d2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+------+\n",
      "|  id|label|               tweet|length|\n",
      "+----+-----+--------------------+------+\n",
      "|   1|  0.0|                test|     4|\n",
      "|  10|  0.0|               photo|     5|\n",
      "|1000|  1.0|ive gone thru fou...|    42|\n",
      "|1001|  0.0|            canteras|     8|\n",
      "|1002|  0.0|                cake|     4|\n",
      "|1004|  0.0|thanks for my del...|    95|\n",
      "|1007|  0.0|rt this if you th...|    27|\n",
      "|1008|  1.0|had my iphone day...|    57|\n",
      "|1009|  0.0|thanks for follow...|    38|\n",
      "|1012|  1.0|anyone know why m...|    73|\n",
      "|1013|  0.0|     iphone birthday|    15|\n",
      "|1014|  0.0|users reached tha...|    37|\n",
      "|1016|  0.0|  easter john clarke|    18|\n",
      "|1018|  1.0|really upset that...|    66|\n",
      "|1020|  1.0|whyyyy you cant d...|    73|\n",
      "|1021|  1.0|the keyboard in t...|   103|\n",
      "|1022|  0.0|      my lovely girl|    14|\n",
      "|1026|  0.0|two more cases to...|    39|\n",
      "|1028|  0.0|soooooo whos a sp...|    59|\n",
      "|1030|  0.0|one of my faves f...|    39|\n",
      "+----+-----+--------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c3af3a7-a6dd-414e-bb62-b4f8910d76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b440e57e-2cda-4bc9-bae0-15867fc599a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/16 19:28:40 WARN StopWordsRemover: Default locale set was [en_EG]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.filter(col(\"tweet\").isNotNull() & (col(\"tweet\") != \"\"))\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"features\")\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashing, lr])\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2a8d1ef2-af1a-402f-b268-6bf883b9dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|               tweet|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|   bado con with and|  0.0|       0.0|\n",
      "|overrated waste o...|  1.0|       1.0|\n",
      "|       like a geisha|  0.0|       0.0|\n",
      "|so i can log in t...|  1.0|       1.0|\n",
      "|effect created st...|  0.0|       0.0|\n",
      "|my apple was so h...|  0.0|       0.0|\n",
      "|johnson will he s...|  0.0|       0.0|\n",
      "|i might have to v...|  1.0|       0.0|\n",
      "|get off when you ...|  0.0|       0.0|\n",
      "|get off apple itu...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)  # or train_df\n",
    "predictions.select(\"tweet\", \"label\", \"prediction\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2893deaa-fa72-4642-8518-5b6783962c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      " [[1502.  215.]\n",
      " [ 189.  365.]]\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "pred_rdd = predictions.select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = MulticlassMetrics(pred_rdd)\n",
    "print(\"Confusion Matrix:\\n\", metrics.confusionMatrix().toArray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ccd3e5c1-3eb6-464f-af11-dc74de5c9aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  id|label|               tweet|length|               words|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+----+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 100|    0|   bado con with and|    17|[bado, con, with,...|         [bado, con]|(262144,[115902,1...|[-24.706156705129...|[0.72867774066642...|       0.0|\n",
      "|1003|    1|overrated waste o...|    58|[overrated, waste...|[overrated, waste...|(262144,[41809,83...|[-74.649254747980...|[0.77430082891837...|       0.0|\n",
      "|1005|    0|       like a geisha|    13|   [like, a, geisha]|      [like, geisha]|(262144,[133592,2...|[-20.464829952558...|[0.85170175244842...|       0.0|\n",
      "|1006|    1|so i can log in t...|    97|[so, i, can, log,...|[log, paypal, and...|(262144,[36987,74...|[-107.92766584354...|[0.01618405317351...|       1.0|\n",
      "| 101|    0|effect created st...|    32|[effect, created,...|[effect, created,...|(262144,[19212,31...|[-35.868233080013...|[0.83954494239423...|       0.0|\n",
      "|1010|    0|my apple was so h...|    32|[my, apple, was, ...|[apple, happy, to...|(262144,[64238,17...|[-32.068796232867...|[0.99816897003244...|       0.0|\n",
      "|1011|    0|johnson will he s...|    34|[johnson, will, h...|[johnson, shine, ...|(262144,[36040,50...|[-36.849062333025...|[0.88698537205016...|       0.0|\n",
      "|1015|    1|i might have to v...|    84|[i, might, have, ...|[might, visit, on...|(262144,[8834,218...|[-103.74679078917...|[0.88743583737965...|       0.0|\n",
      "|1017|    0|get off when you ...|    80|[get, off, when, ...|[get, order, hard...|(262144,[1512,243...|[-88.914748908229...|[0.99996198501699...|       0.0|\n",
      "|1019|    0|get off apple itu...|    38|[get, off, apple,...|[get, apple, itun...|(262144,[16166,22...|[-57.204488791460...|[0.94008390297086...|       0.0|\n",
      "| 102|    0|finalllllly my no...|    27|[finalllllly, my,...| [finalllllly, note]|(262144,[43157,78...|[-22.032008055702...|[0.89641357218549...|       0.0|\n",
      "|1023|    0|mini gb wifi now ...|    65|[mini, gb, wifi, ...|[mini, gb, wifi, ...|(262144,[3969,186...|[-92.642116633517...|[0.99869358000464...|       0.0|\n",
      "|1024|    0|             fitness|     7|           [fitness]|           [fitness]|(262144,[136835],...|[-12.157785969125...|[0.84648534685320...|       0.0|\n",
      "|1025|    1|you cant handle y...|    47|[you, cant, handl...|[cant, handle, wi...|(262144,[29484,13...|[-56.821839825630...|[0.16569083118700...|       1.0|\n",
      "|1027|    0|iphone charger ca...|    63|[iphone, charger,...|[iphone, charger,...|(262144,[8465,317...|[-106.95915878755...|[0.95659858741957...|       0.0|\n",
      "|1029|    0|that hitler is al...|    46|[that, hitler, is...|[hitler, always, ...|(262144,[57994,79...|[-45.777546486402...|[0.66466634105788...|       0.0|\n",
      "| 103|    0|special app for v...|    30|[special, app, fo...|[special, app, va...|(262144,[20575,49...|[-39.728992972310...|[0.99894218948519...|       0.0|\n",
      "|1033|    0|mi nuevo juguete ...|    37|[mi, nuevo, jugue...|[mi, nuevo, jugue...|(262144,[89833,95...|[-64.017074376637...|[0.98777792047869...|       0.0|\n",
      "|1036|    0|     black and white|    15| [black, and, white]|      [black, white]|(262144,[75571,15...|[-20.001141184171...|[0.98018263544808...|       0.0|\n",
      "|1037|    1|i need a samsung ...|   184|[i, need, a, sams...|[need, samsung, n...|(262144,[6122,223...|[-167.94216828076...|[0.99996638653444...|       0.0|\n",
      "+----+-----+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/14 15:23:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18e2aa85-8234-4d0b-8b76-a5d6a6cf066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = predictions.select(\"id\", \"tweet\", \"label\", \"prediction\", \"probability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d3a517a-f53a-46d6-b76f-6e5febb86abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/16 19:07:08 WARN StopWordsRemover: Default locale set was [en_EG]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               tweet|      filtered_words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                test|              [test]|(1000,[586],[5.57...|\n",
      "|finally a transpa...|[finally, transpa...|(1000,[161,229,34...|\n",
      "|we love this woul...|          [love, go]|(1000,[240,451],[...|\n",
      "|im wired i know i...|[im, wired, know,...|(1000,[344,349,39...|\n",
      "|what amazing serv...|[amazing, service...|(1000,[0,44,217,2...|\n",
      "|iphone software u...|[iphone, software...|(1000,[0,48,66,15...|\n",
      "|        happy for us|         [happy, us]|(1000,[347,660],[...|\n",
      "|new type c charge...|[new, type, c, ch...|(1000,[34,357,526...|\n",
      "|bout to go shoppi...|[bout, go, shoppi...|(1000,[370,409,45...|\n",
      "|               photo|             [photo]|(1000,[367],[2.84...|\n",
      "|hey when you make...|[hey, make, new, ...|(1000,[54,64,92,2...|\n",
      "|ha not heavy mach...|[ha, heavy, machi...|(1000,[112,492,53...|\n",
      "|contemplating giv...|[contemplating, g...|(1000,[103,509,58...|\n",
      "|i just made anoth...|[made, another, c...|(1000,[125,144,16...|\n",
      "|the battery is so...|[battery, painful...|(1000,[298,309,39...|\n",
      "|        from towards|           [towards]|(1000,[361],[5.21...|\n",
      "|like and share if...|[like, share, wan...|(1000,[330,342,62...|\n",
      "|            go crazy|         [go, crazy]|(1000,[144,451],[...|\n",
      "|the reason i dont...|[reason, dont, on...|(1000,[528,815,94...|\n",
      "|how is the apple ...|[apple, store, gu...|(1000,[184,493,53...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Tokenize tweets\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "\n",
    "# 2. Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "\n",
    "# 3. HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "\n",
    "# 4. IDF (to weight the features)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "# Fit and transform\n",
    "model = pipeline.fit(df)       # fit = learn IDF weights\n",
    "new   = model.transform(df)    # transform = apply all stages\n",
    "\n",
    "# Show results\n",
    "new.select(\"tweet\", \"filtered_words\", \"features\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "baf8c40c-b1d4-479c-9e36-a038d4844001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "pdf = predictions.select(\"tweet\", \"label\", \"prediction\", \"probability\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7b3053a6-e1ad-4ae8-8153-69ee378dca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"predictions.csv\"\n",
    "pdf.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c06c06a2-08b3-4194-ad8b-3f7750a0344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to S3!\n",
      "Your Buckets:\n",
      " - omar93tweetdata\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    response = s3.list_buckets()\n",
    "    print(\"✅ Connected to S3!\")\n",
    "    print(\"Your Buckets:\")\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        print(f\" - {bucket['Name']}\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Not connected to S3:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "150e3d68-82e3-42bd-958c-5108effd4738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File uploaded to S3 successfully!\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"omar93tweetdata\"\n",
    "s3.upload_file(csv_path, bucket_name, \"predictions.csv\")\n",
    "\n",
    "print(\"✅ File uploaded to S3 successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748daf0-af7f-4853-8a42-2cb502711468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
